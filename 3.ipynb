{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f0a19cd",
   "metadata": {},
   "source": [
    "Nominal encoding, also known as label encoding, is preferred over one-hot encoding in certain situations, particularly when dealing with categorical data that doesn't have an inherent order or ranking. Here are some scenarios where nominal encoding is more suitable:\n",
    "\n",
    "Situations Favoring Nominal Encoding\n",
    "Memory Efficiency: When the number of categories is large, one-hot encoding can significantly expand the feature space, leading to a high-dimensional dataset. Nominal encoding is more memory-efficient as it creates a single new column with integer values.\n",
    "\n",
    "Tree-Based Models: Machine learning models like decision trees or random forests can handle categorical variables in their native form more effectively. Nominal encoding is often suitable for these models as they can handle the integer encoded categories without the need for expansive binary features.\n",
    "\n",
    "Avoiding the Dummy Variable Trap: One-hot encoding can lead to multicollinearity due to the addition of multiple correlated features (a problem known as the dummy variable trap). Nominal encoding avoids this by creating only one feature.\n",
    "\n",
    "Simplicity: In some scenarios, simplicity and ease of interpretation are more important. Nominal encoding keeps the dataset more compact and easier to interpret.\n",
    "\n",
    "Practical Example\n",
    "Consider a dataset of cars where one of the features is the \"brand\" of the car (e.g., Toyota, Ford, BMW, etc.). Assume there are 50 different brands. Using one-hot encoding here would create 50 additional features, each representing a brand. This can significantly increase the dimensionality of the dataset, leading to potential issues like overfitting and increased computational cost.\n",
    "\n",
    "If a tree-based model is used, applying nominal encoding would be more efficient. Each brand could be assigned a unique integer (e.g., Toyota = 1, Ford = 2, BMW = 3, etc.). This approach keeps the data dimensionality in check and is more memory-efficient. Since tree-based models can inherently handle categorical data and their splits, the integer representation would not hinder the model's performance and might even lead to faster training times."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
